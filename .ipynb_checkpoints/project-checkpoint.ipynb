{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7dc1b1",
   "metadata": {},
   "source": [
    "# 3D Computer Vision (2024/25)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Upload: 30.10.2024 (11:30)\n",
    "\n",
    "**Deadline**: 16.01.2025 (23:59)\n",
    "\n",
    "### Group 07 \n",
    "- Daiana Tei\n",
    "- Dhruv Kale\n",
    "- Name3\n",
    "- Name4\n",
    "\n",
    "By submitting this exercise, we confirm the following:\n",
    "- **All people** listed above **contributed** to this solution\n",
    "- **No other people** were **involved** in this solution\n",
    "- **No contents** of this solution were **copied from others** (this includes people, large language models, websites, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337c5fd",
   "metadata": {},
   "source": [
    "# 3D Scene Reconstruction\n",
    "### Task Overview\n",
    "Your task in this exercise is to do a dense reconstruction of a scene. This will involve multiple steps that you will encounter and learn about as the semester progresses. You can start implementing individual steps as soon as you learn about them or wait until you have learned more to implement everything together. In the latter case, be mindful that this exercise is designed for an entire semester and the workload is accordingly large.\n",
    "\n",
    "You will be given the following data:\n",
    "- **9 color images** of the scene.\n",
    "    - 8 Bit RGB per pixel.\n",
    "    - Each image rendered from a different position.\n",
    "    - The camera used had **lens distortion**.\n",
    "- **9 Depth images** of the scene.\n",
    "    - 8 Bit Grayscale per pixel. The result of dividing the Z-depth by **each image's maximum** and then multiplying by 255.\n",
    "    - Each image has the **same pose** as the corresponding RGB image.\n",
    "    - The camera used was **free of any distortions**.\n",
    "- 1 Dictionary containing **camera calibration parameters**.\n",
    "    - They belong to the camera that was used to render the RGB images.\n",
    "    - Distortion coefficients are given in the standard [k<sub>1</sub>, k<sub>2</sub>, p<sub>1</sub>, p<sub>2</sub>, k<sub>3</sub>] order.\n",
    "- 1 Numpy array containing **8 camera transformations**.\n",
    "    - They specify the **movements** that the **camera went through** to render all images.\n",
    "    - I.e. idx **0** specifies the transformation from **00.png to 01.png**, idx **1** specifies the transformation from **01.png to 02.png**, ...\n",
    "    - This applies to both RGB and Depth images, as they have the same poses.\n",
    "- 1 Numpy array containing **7 features**.\n",
    "    - The features are specified for each of the 9 images.\n",
    "    - Each feature is a **2D pixel location in \"H, W\" order**, meaning the first value is the height/row in the image and the second width/column.\n",
    "    - If a feature was not visible, it was entered as [-1, -1].\n",
    "    - The features are **unsorted**, meaning that feature idx 0 for 00.png could be corresponding to e.g. feature idx 4 for 01.png.\n",
    "\n",
    "### Solution requirements\n",
    "- Your code needs to **compile**, **run**, and produce an **output**.\n",
    "- Your target output should be a **dense point cloud** reconstruction (without holes) of the scene.\n",
    "    - The output should be in the **.ply format**. We provide a function that can exports a .ply file.\n",
    "    - You may inspect your .ply outputs in e.g. **Meshlab**.\n",
    "    - See the 'Dense Point Cloud' sample image to get an idea of what is possible. (Meshlab screenshot with point shading set to None)\n",
    "- Your code should be a **general solution**.\n",
    "    - This means that it could run correctly for a different dataset (with same input structure).\n",
    "    - It should **NOT** include anything **hardcoded** specific to this dataset.\n",
    "- Your code should not be unnecessarily inefficient.\n",
    "    - Our sample solution runs in less than 2 minutes total (including point cloud export).\n",
    "    - If your solution runs for more than 10 minutes, you are being wasteful in some part of your program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ab8f9c-4fd8-47fd-b7a6-fe2028849ae2",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bde4af",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Please note the following:\n",
    "- These are all imports necessary to achieve the sample results.\n",
    "- You may remove and/or add other libraries at your own convinience.\n",
    "- Using library functions (from the given or other libraries) that bypass **necessary computer vision tasks** will not be recognized as 'solved'.\n",
    "    - E.g.: If you **need** to undistort an image to **get to the next step** of the solution and use the library function cv2.undistort(), then we will evaluate the **undistortion step** as '**failed**'.\n",
    "    - E.g.: If you **want** to draw points in an image (to **check your method** or **visualize in-between steps**) and use the library function cv2.circle(), then there is **no problem**.\n",
    "    - E.g.: If you **need** to perform complex **mathematical** operations and use some numpy function, then there is **no problem**.\n",
    "    - E.g.: You do not like a **provided utility function** and find/know a library function that gives the **same outputs** from the **same inputs**, then there is **no problem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d17d5-d4f4-4be1-b058-772b0cf468c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import cv2\n",
    "#import open3d as o3d\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68e1e4",
   "metadata": {},
   "source": [
    "# Data\n",
    "This should load all available data and also create some output directories. Feel free to rename variables or add additional directories as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85509287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "base_path = os.getcwd()\n",
    "data_path = os.path.join(base_path, f\"data\")\n",
    "img_path = os.path.join(data_path, 'images')\n",
    "depth_path = os.path.join(data_path, 'depths')\n",
    "print(f\"The project's root path is '{base_path}'.\")\n",
    "print(f\"Reading data from '{data_path}'.\")\n",
    "print(f\"Image folder: '{img_path}'.\")\n",
    "print(f\"Depth folder: '{depth_path}'.\")\n",
    "\n",
    "#Outputs\n",
    "out_path = os.path.join(base_path, 'output')\n",
    "ply_path = os.path.join(out_path, 'point_cloud')\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "os.makedirs(ply_path, exist_ok=True)\n",
    "print(f\"\\nCreating directory '{out_path}'.\")\n",
    "print(f\"Creating directory '{ply_path}'.\")\n",
    "\n",
    "#Load Data\n",
    "camera_calibration = np.load(os.path.join(data_path, 'camera_calibration.npy'), allow_pickle=True)\n",
    "camera_calibration = camera_calibration.item()#get dictionary from numpy array struct\n",
    "given_features = np.load(os.path.join(data_path, 'given_features.npy'), allow_pickle=True)\n",
    "camera_movement = np.load(os.path.join(data_path, 'camera_movement.npy'), allow_pickle=True)\n",
    "\n",
    "print(f\"\\nCamera Calibration:\")\n",
    "for entry in camera_calibration.items():\n",
    "    print(f\"  {entry[0]}: {entry[1]}\")\n",
    "print(f\"Camera Movement: {camera_movement.shape}\")#[Cameras-1, 4, 4]\n",
    "print(f\"2D Features (Unsorted): {given_features.shape}\")#[Camera_idx, Feature_idx, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd8870b-c64c-4a42-888e-387c81e27291",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "rgb_paths = sorted(glob.glob(\"data/images/*.png\"))\n",
    "depth_paths = sorted(glob.glob(\"data/depths/*.png\"))\n",
    "feature_data = np.load(\"data/given_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4b331b-8573-471a-bbb3-89eeb0ee80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb5d16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(rgb_paths)):\n",
    "    rgb_image = cv2.imread(rgb_paths[i])\n",
    "    depth_map = cv2.imread(depth_paths[i], cv2.IMREAD_GRAYSCALE)\n",
    "    features = feature_data[i]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"RGB Image {i}\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(depth_map, cmap=\"jet_r\")\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Depth Map {i}\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB))\n",
    "    for feature in features:\n",
    "        if feature[0] != -1:  # invalid features: if feature is not visible in image, coordinates are [-1, -1]\n",
    "            plt.plot(feature[1], feature[0], 'ro', markersize=2)\n",
    "    plt.title(f\"Features {i}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9cbc5b44-f7ce-4136-8eed-201fca12f9a5",
   "metadata": {},
   "source": [
    "--- Data visualisation prepared by Daiana Tei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491e536",
   "metadata": {},
   "source": [
    "# Provided Utility Functions\n",
    "These functions are provided to reduce the complexity of some steps you might encounter. They were involved in the creation of the given samples. However, you do not have to use them and can use other means of achieving the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e69911b-e5df-49dc-8dd0-37a8af1c1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(numpy_image, numpy_sample_grid):\n",
    "    '''\n",
    "    This function samples a target image from a source image (numpy_image) based on specified pixel coordinates (numpy_sample_grid).\n",
    "    Inputs:\n",
    "        numpy_image: of shape=[H, W, C]. H is the height, W is the width, and C is the color channel of the source image from which color values will be sampled.\n",
    "        numpy_sample_grid: of shape=[H, W, UV]. H is the height and W is the width of the target image that will be sampled. UV are the pixel locations in the source image from which to sample color values.\n",
    "    Outputs:\n",
    "        sampled_image: of shape=[H, W, C]. H is the height, W is the width, and C is the color channel of the target image that was sampled.\n",
    "    '''\n",
    "    height, width, _ = numpy_image.shape#[H, W, 3]\n",
    "\n",
    "    #turn numpy array to torch tensor\n",
    "    torch_sample_grid = torch.from_numpy(numpy_sample_grid)#[H, W, 2]\n",
    "    #normalize from range (0, width-1) to (0, 1)\n",
    "    torch_sample_grid[:, :, 0] = torch_sample_grid[:, :, 0] / (width-1)\n",
    "    #normalize from range (0, height-1) to (0, 1)\n",
    "    torch_sample_grid[:, :, 1] = torch_sample_grid[:, :, 1] / (height-1)\n",
    "    #normalize from range (0, 1) to (-1, 1)\n",
    "    torch_sample_grid = torch_sample_grid*2 -1\n",
    "\n",
    "    #transform to necessary shapes\n",
    "    torch_sample_grid = torch_sample_grid.unsqueeze(0)#[1, H, W, 2]\n",
    "    torch_image = torch.from_numpy(numpy_image).double().permute(2, 0, 1).unsqueeze(0)#[1, 3, H, W]\n",
    "    #sample image according to sample grid locations from source image\n",
    "    sampled_image = torch.nn.functional.grid_sample(torch_image, torch_sample_grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "    #transform back to numpy image\n",
    "    sampled_image = sampled_image.squeeze().permute(1, 2, 0).numpy().astype(np.uint8)#[H, W, 3]\n",
    "    return sampled_image\n",
    "\n",
    "def ply_creator(input_3d, rgb_data=None, filename='dummy'):\n",
    "    ''' Creates a colored point cloud that you can visualise using e.g. Meshlab.\n",
    "    Inputs:\n",
    "        input_3d: of shape=[N, 3], each row is the 3D coordinate of each point\n",
    "        rgb_data(optional): of shape=[N, 3], each row is the rgb color value of each point\n",
    "        filename: file name for the .ply file to be created \n",
    "    '''\n",
    "    assert (input_3d.ndim==2),\"Pass 3d points as NumPointsX3 array \"\n",
    "    pre_text1 = \"\"\"ply\\nformat ascii 1.0\"\"\"\n",
    "    pre_text2 = \"element vertex \"\n",
    "    pre_text3 = \"\"\"property float x\\nproperty float y\\nproperty float z\\nproperty uchar red\\nproperty uchar green\\nproperty uchar blue\\nend_header\"\"\"\n",
    "    pre_text22 = pre_text2 + str(input_3d.shape[0])\n",
    "    pre_text11 = pre_text1\n",
    "    pre_text33 = pre_text3\n",
    "    filename = filename + '.ply'\n",
    "    fid = open(filename, 'w')\n",
    "    fid.write(pre_text11)\n",
    "    fid.write('\\n')\n",
    "    fid.write(pre_text22)\n",
    "    fid.write('\\n')\n",
    "    fid.write(pre_text33)\n",
    "    fid.write('\\n')\n",
    "    for i in range(input_3d.shape[0]):\n",
    "        for c in range(3):\n",
    "            fid.write(str(input_3d[i,c]) + ' ')\n",
    "        if not rgb_data is None:\n",
    "            for c in range(3):\n",
    "                fid.write(str(rgb_data[i,c]) + ' ')\n",
    "        if i!=input_3d.shape[0]:\n",
    "            fid.write('\\n')\n",
    "    fid.close()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following is a simple toy example to see the behavior of the sample_image function\n",
    "example_source = (np.random.rand(110, 220, 3)*255).astype(int)#[110, 220, 3]\n",
    "example_grid = np.ones([100, 200, 2])#[100, 200, 2]\n",
    "example_grid[:, :, 1] = 2\n",
    "example_target = sample_image(example_source, example_grid)#[100, 200, 3]\n",
    "\n",
    "#example_target will be of shape [100, 200, 3]\n",
    "#100, because example_grid has a height of 100\n",
    "#200, because example_grid has a width of 200\n",
    "#3, because example_source has a color channel of 3\n",
    "print(example_source.shape)\n",
    "\n",
    "#example_target will contain the value of example_source[2, 1] at every pixel\n",
    "#2, because example_grid[:, :, 1] has a value of 2 for every pixel\n",
    "#1, because example_grid[:, :, 0] has a value of 1 for every pixel\n",
    "print(example_source[2, 1, 0], \"->\", np.unique(example_target[:, :, 0]))\n",
    "print(example_source[2, 1, 1], \"->\", np.unique(example_target[:, :, 1]))\n",
    "print(example_source[2, 1, 2], \"->\", np.unique(example_target[:, :, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa9914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51bd3691-c4a5-4dc6-85c4-c66932a2acf7",
   "metadata": {},
   "source": [
    "# Undistortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ab37d-b50c-45b3-9d28-edde436d27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "focal_length_px = camera_calibration['focal_length_px']\n",
    "principal_point = camera_calibration['principal_point']\n",
    "coeffs = camera_calibration['distortion_param']\n",
    "\n",
    "K = np.array([\n",
    "    [focal_length_px, 0, principal_point[1]],  \n",
    "    [0, focal_length_px, principal_point[0]],  \n",
    "    [0, 0, 1]                                 \n",
    "], dtype=np.float64)\n",
    "\n",
    "print(\"Camera matrix K:\")\n",
    "print(K)\n",
    "print(\"\\nDistortion coefficients:\")\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d757faca-6a47-4093-9c0d-1c930a85ab0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def undistort_image_inverse_sampling(image, file_name, K, distortion_params):\n",
    "    h, w = image.shape[:2]\n",
    "    f_x, f_y = K[0, 0], K[1, 1]\n",
    "    u_0, v_0 = K[0, 2], K[1, 2]\n",
    "\n",
    "    # empty output\n",
    "    undistorted = np.zeros_like(image)\n",
    "    # grid of undistorted coordinates\n",
    "    u_grid, v_grid = np.meshgrid(np.arange(w), np.arange(h))\n",
    "    undistorted_coords = np.stack([u_grid.ravel(), v_grid.ravel()], axis=-1)\n",
    "    distorted_coords = []\n",
    "    \n",
    "    # for each coordinate, calculate corresponding distorted coordinates\n",
    "    for u, v in undistorted_coords:\n",
    "        # normalisation\n",
    "        x = (u - u_0) / f_x\n",
    "        y = (v - v_0) / f_y\n",
    "        # invert distortion model\n",
    "        x_dist, y_dist = x, y\n",
    "        \n",
    "        r2 = x_dist**2 + y_dist**2\n",
    "        radial_distortion = 1 + distortion_params[0]*r2 + distortion_params[1]*r2**2 + distortion_params[4]*r2**3\n",
    "        tangential_x = 2 * distortion_params[2] * x_dist * y_dist + distortion_params[3] * (r2 + 2 * x_dist**2)\n",
    "        tangential_y = distortion_params[2] * (r2 + 2 * y_dist**2) + 2 * distortion_params[3] * x_dist * y_dist\n",
    "\n",
    "        x_dist = x * radial_distortion + tangential_x\n",
    "        y_dist = y * radial_distortion + tangential_y\n",
    "\n",
    "        # map distorted normalized coordinates back to pixels\n",
    "        u_dist = f_x * x_dist + u_0\n",
    "        v_dist = f_y * y_dist + v_0\n",
    "        distorted_coords.append((u_dist, v_dist))\n",
    "\n",
    "    distorted_coords = np.array(distorted_coords).reshape(h, w, 2)\n",
    "    # bilinear interpolation to sample distorted image\n",
    "    map_x = distorted_coords[:, :, 0].astype(np.float32)\n",
    "    map_y = distorted_coords[:, :, 1].astype(np.float32)\n",
    "    undistorted = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n",
    "    \n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_path = os.path.join(output_folder, file_name)\n",
    "    cv2.imwrite(output_path, undistorted)\n",
    "    print(f\"Saved undistorted image to {output_path}\")\n",
    "    return undistorted\n",
    "\n",
    "for i in range(len(rgb_paths)):\n",
    "    distorted_image = cv2.imread(rgb_paths[i])\n",
    "    undistorted_image = undistort_image_inverse_sampling(distorted_image, f\"{i}.png\", K, coeffs)\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(distorted_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Distorted Image\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(undistorted_image, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Undistorted Image\")\n",
    "    plt.show()\n",
    "\n",
    "undistorted_images = sorted(glob.glob(output_folder+\"/*.png\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b359499-dd31-4467-aca8-00bfcb4f0e40",
   "metadata": {},
   "source": [
    "--- Undistortion prepared by Daiana Tei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2cb26-81ff-47e7-83c6-ddb7ade173c9",
   "metadata": {},
   "source": [
    "# Matching features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e809c8e6-93c3-4b68-84aa-0ba1b33f2be2",
   "metadata": {},
   "source": [
    "### Data observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7d356-a06d-4a57-ac5a-31af3d55cfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feature_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4804bf44-67b8-4fc1-8292-5b73977fade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(undistorted_images)):\n",
    "    rgb_image = cv2.imread(undistorted_images[i])\n",
    "    features = feature_data[i]\n",
    "    \n",
    "    plt.imshow(cv2.cvtColor(rgb_image, cv2.COLOR_BGR2RGB))\n",
    "    for feature in features:\n",
    "        if feature[0] != -1:  # invalid features: if feature is not visible in image, coordinates are [-1, -1]\n",
    "            plt.plot(feature[1], feature[0], 'ro', markersize=2)\n",
    "    plt.title(f\"Features {i}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccffdfe-9835-4e4f-a01e-3a8e52bdad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(camera_movement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759bce63-c377-439d-ab1f-db12fe5e6c79",
   "metadata": {},
   "source": [
    "### Camera transformations visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c3c493-600c-4ac8-9b8a-76f4e0721743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def look_at(eye, target, up):\n",
    "    eye = np.array(eye, dtype=np.float64)  \n",
    "    target = np.array(target, dtype=np.float64)\n",
    "    up = np.array(up, dtype=np.float64)\n",
    "    \n",
    "    forward = target - eye\n",
    "    forward /= np.linalg.norm(forward) \n",
    "    right = np.cross(up, forward)\n",
    "    right /= np.linalg.norm(right) \n",
    "    up = np.cross(forward, right)\n",
    "\n",
    "    rotation_matrix = np.eye(4)\n",
    "    rotation_matrix[:3, 0] = right\n",
    "    rotation_matrix[:3, 1] = up\n",
    "    rotation_matrix[:3, 2] = -forward\n",
    "\n",
    "    translation_matrix = np.eye(4)\n",
    "    translation_matrix[:3, 3] = -eye\n",
    "    return rotation_matrix @ translation_matrix\n",
    "\n",
    "# Initial position\n",
    "eye = [0, 0, 0]\n",
    "target = [1, 1, 0]\n",
    "up = [0, 0, 1]\n",
    "initial_pose = look_at(eye, target, up)\n",
    "\n",
    "camera_poses = [initial_pose]\n",
    "current_pose = initial_pose.copy()\n",
    "for transform in camera_movement:\n",
    "    current_pose = current_pose @ transform\n",
    "    camera_poses.append(current_pose)\n",
    "\n",
    "camera_positions = np.array([pose[:3, 3] for pose in camera_poses])\n",
    "camera_rotations = np.array([pose[:3, :3] for pose in camera_poses])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(camera_positions[:, 0], camera_positions[:, 1], camera_positions[:, 2], c='r')\n",
    "\n",
    "for i, (pose, R) in enumerate(zip(camera_positions, camera_rotations)):\n",
    "    x_axis = R[:, 0]\n",
    "    y_axis = R[:, 1]\n",
    "    z_axis = R[:, 2]\n",
    "    start = pose\n",
    "    length = 0.5\n",
    "    ax.quiver(start[0], start[1], start[2], x_axis[0], x_axis[1], x_axis[2], color='r', length=length)\n",
    "    ax.quiver(start[0], start[1], start[2], y_axis[0], y_axis[1], y_axis[2], color='g', length=length)\n",
    "    ax.quiver(start[0], start[1], start[2], z_axis[0], z_axis[1], z_axis[2], color='b', length=length)\n",
    "    ax.text(start[0], start[1], start[2], f' {i}', None)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b7576e0-1a2f-4fc7-b9b4-82543efef8af",
   "metadata": {},
   "source": [
    "--- Visualisation prepared by Daiana Tei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b13a6-33fa-4e8b-abf5-853cd4dc1cfd",
   "metadata": {},
   "source": [
    "# Feature matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a2dccf-da41-4e6b-b225-4adbd702ca3e",
   "metadata": {},
   "source": [
    "### Computing fundamental matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0ab974-bd2f-4611-b5cc-f35ac86412fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_essential_matrix(R, t):\n",
    "    \"\"\"\n",
    "    Computes essential matrix E (3x3) from rotation matrix (3x3) and translation vector (3,).\n",
    "    \"\"\"\n",
    "    # Skew-symmetric matrix for t\n",
    "    t_skew = np.array([\n",
    "        [0, -t[2], t[1]],\n",
    "        [t[2], 0, -t[0]],\n",
    "        [-t[1], t[0], 0]\n",
    "    ])\n",
    "    # E = [t]_x R\n",
    "    E = t_skew @ R\n",
    "    return E\n",
    "    \n",
    "def get_fundamental_matrix(E, K):\n",
    "    \"\"\"\n",
    "    Computes fundamental matrix F (3x3) from essential matrix E (3x3) and camera matrix K (3x3).\n",
    "    \"\"\"\n",
    "    # F = K^-T E K^-1\n",
    "    F = np.linalg.inv(K).T @ E @ np.linalg.inv(K)\n",
    "    # Normalisation\n",
    "    F = F / F[2, 2]\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904897be-fe75-4042-9d43-581e087fa255",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(undistorted_images)\n",
    "\n",
    "F = []\n",
    "for i in range(num_images-1):\n",
    "    R = camera_movement[i][:3, :3]\n",
    "    t = camera_movement[i][:3, 3]\n",
    "    E = get_essential_matrix(R, t)\n",
    "    F.append(get_fundamental_matrix(E, K))\n",
    "\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ef4cda-fb10-470a-8a15-08dcad71d935",
   "metadata": {},
   "source": [
    "### Projecting epipolar lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c7f05-f114-43a2-bd3d-4a8bf1e8d108",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k'] # Matplotlib color codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6f35fa-203b-44b6-8075-b6381fbbb28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_images-1):\n",
    "    img1 = cv2.cvtColor(cv2.imread(undistorted_images[i]), cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(cv2.imread(undistorted_images[i+1]), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # Features in first image\n",
    "    axs[0].imshow(img1)\n",
    "    k = 0\n",
    "    for p1 in feature_data[i]:\n",
    "        if p1[0] == -1:\n",
    "            continue\n",
    "        axs[0].plot(p1[1], p1[0], colors[k] + 'o', markersize=3) # height is x, width is y\n",
    "        k += 1\n",
    "\n",
    "    axs[1].imshow(img2)\n",
    "    # Epipolar lines from features in second image using respective colours\n",
    "    k = 0\n",
    "    for p1 in feature_data[i]:\n",
    "        if p1[0] == -1:\n",
    "            continue\n",
    "        x1 = np.array([p1[1], p1[0], 1])  # homogeneous coordinates\n",
    "        l2 = F[i] @ x1  # epipolar line\n",
    "        l2 = [l2[0]/l2[2], l2[1]/l2[2], 1.0]\n",
    "        \n",
    "        h, w = img2.shape[:2]\n",
    "        x0, y0 = 0, -l2[2] / l2[1]\n",
    "        x1, y1 = h, -(l2[2] + l2[0] * h) / l2[1]\n",
    "        axs[1].axline((x1, y1), (x0, y0), color=colors[k])\n",
    "        k += 1\n",
    "\n",
    "    # Features in second image\n",
    "    for p1 in feature_data[i+1]:\n",
    "        if p1[0] == -1:\n",
    "            continue\n",
    "        axs[1].plot(p1[1], p1[0], 'ro', markersize=3) \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42c7f108-2cd0-4649-a9ac-b15048a91ac4",
   "metadata": {},
   "source": [
    "--- Feature matching prepared by Daiana Tei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cec079-2390-48fb-a78b-7775a9e92932",
   "metadata": {},
   "source": [
    "### Finding correspondences between consecutive images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78543c0-9e1b-4e23-abf9-7b5ab7ed5762",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(feature_data[0])\n",
    "\n",
    "def match_feature_(feature, features_to_match, F, distance_threshold=2):\n",
    "    \"\"\"\n",
    "    Matches feature to that of another view.\n",
    "    \"\"\"\n",
    "    if feature[0] == -1:\n",
    "        return -1\n",
    "\n",
    "    x1 = np.array([feature[1], feature[0], 1])\n",
    "    epipolar_line = F @ x1\n",
    "    epipolar_line = [epipolar_line[0]/epipolar_line[2], epipolar_line[1]/epipolar_line[2], 1.0]\n",
    "    best_match = -1\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    for l in range(len(features_to_match)):\n",
    "        if features_to_match[l, 0] != -1:\n",
    "            x2 = np.array([features_to_match[l, 1], features_to_match[l, 0], 1])\n",
    "            distance = np.abs(epipolar_line[0] * x2[0] + epipolar_line[1] * x2[1] + epipolar_line[2]) / np.sqrt(epipolar_line[0]**2 + epipolar_line[1]**2)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                best_match = l\n",
    "\n",
    "    if min_distance < distance_threshold:\n",
    "        return best_match\n",
    "    return -1\n",
    "\n",
    "matches = []\n",
    "for i in range(num_images-1):\n",
    "    matches_i = []\n",
    "    for j in range(num_features):\n",
    "        match = match_feature_(feature_data[i, j], feature_data[i+1], F[i])\n",
    "        if match != -1:\n",
    "            matches_i.append((j, match))\n",
    "    matches.append(matches_i)\n",
    "    print(f\"{i}->{i+1}\", matches_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e9765-d355-4ef4-99ba-f017e470d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_images-1):\n",
    "    img1 = cv2.cvtColor(cv2.imread(undistorted_images[i]), cv2.COLOR_BGR2RGB)\n",
    "    img2 = cv2.cvtColor(cv2.imread(undistorted_images[i+1]), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axs[0].imshow(img1)\n",
    "    for j, (match1, match2) in enumerate(matches[i]):\n",
    "        u1, v1 = feature_data[i, match1]\n",
    "        axs[0].plot(v1, u1, colors[j] + 'o', markersize=3) # Note: v1 is x, u1 is y for plotting\n",
    "\n",
    "    axs[1].imshow(img2)\n",
    "    for j, (match1, match2) in enumerate(matches[i]):\n",
    "        u2, v2 = feature_data[i+1, match2]\n",
    "        axs[1].plot(v2, u2, colors[j] + 'o', markersize=3)  # Note: v2 is x, u2 is y\n",
    "\n",
    "    axs[0].set_title(f\"Image {i}\")\n",
    "    axs[1].set_title(f\"Image {i+1}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "708d38a4-02ab-4064-9b1e-214317920e25",
   "metadata": {},
   "source": [
    "--- Feature matching prepared by Daiana Tei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d306183-2ef7-4c9b-bfca-5310138c9ddf",
   "metadata": {},
   "source": [
    "### Visualising all correspondences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d69bf0-accc-424f-a28c-333b8f6b3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transformation_matrices(view_idx):\n",
    "    \"\"\"\n",
    "    Calculates transformation matrices from the given view to all views\n",
    "    \"\"\"\n",
    "    pose = np.eye(4)\n",
    "    transformations = [pose]\n",
    "    for i in range(num_images-1 - view_idx):\n",
    "        pose = camera_movement[i+view_idx] @ pose\n",
    "        transformations.append(pose)\n",
    "\n",
    "    pose = np.eye(4)\n",
    "    for i in range(view_idx):\n",
    "        pose = np.linalg.inv(camera_movement[view_idx-1-i]) @ pose\n",
    "        transformations.insert(0, pose)\n",
    "    return transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6b894-3d38-4cb2-815d-94efbf99f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features_from_view(idx):\n",
    "    transformations = get_transformation_matrices(idx)\n",
    "    for i in range(num_images):\n",
    "        if i == idx:\n",
    "            continue\n",
    "        \n",
    "        img1 = cv2.cvtColor(cv2.imread(undistorted_images[idx]), cv2.COLOR_BGR2RGB)\n",
    "        img2 = cv2.cvtColor(cv2.imread(undistorted_images[i]), cv2.COLOR_BGR2RGB)\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        \n",
    "        axs[0].imshow(img1)\n",
    "        k = 0\n",
    "        for p1 in feature_data[idx]:\n",
    "            if p1[0] == -1:\n",
    "                continue    \n",
    "            axs[0].plot(p1[1], p1[0], colors[k] + 'o', markersize=3) # height is x, width is y\n",
    "            k += 1\n",
    "    \n",
    "        R = transformations[i][:3, :3]\n",
    "        t = transformations[i][:3, 3]\n",
    "        E = get_essential_matrix(R, t)\n",
    "        F = get_fundamental_matrix(E, K)\n",
    "        \n",
    "        axs[1].imshow(img2)\n",
    "        k = 0\n",
    "        bad_pair = False\n",
    "        for p1 in feature_data[idx]:\n",
    "            if p1[0] == -1:\n",
    "                continue\n",
    "            \n",
    "            x1 = np.array([p1[1], p1[0], 1])\n",
    "            l2 = F @ x1\n",
    "            l2 = [l2[0]/l2[2], l2[1]/l2[2], 1.0]\n",
    "            \n",
    "            h, w = img2.shape[:2]\n",
    "            x0, y0 = 0, -l2[2] / l2[1]\n",
    "            x1, y1 = h, -(l2[2] + l2[0] * h) / l2[1]\n",
    "            axs[1].axline((x1, y1), (x0, y0), color=colors[k])\n",
    "            k += 1\n",
    "    \n",
    "            if abs(y0) > (h+w) * 2 or abs(y1) > (h+w) * 2:\n",
    "                bad_pair = True\n",
    "                break\n",
    "    \n",
    "        if bad_pair == True:\n",
    "            print(f\"! ! ! BAD VIEWS between images {idx} and {i}\")\n",
    "            continue\n",
    "    \n",
    "        for p1 in feature_data[i]:\n",
    "            if p1[0] == -1:\n",
    "                continue\n",
    "            axs[1].plot(p1[1], p1[0], 'ro', markersize=3) \n",
    "    \n",
    "        axs[0].set_title(f\"Image {idx}\")\n",
    "        axs[1].set_title(f\"Image {i}\")\n",
    "        plt.show()\n",
    "\n",
    "for i in range(num_images):\n",
    "    show_features_from_view(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba898e02-76d2-4a11-90f2-83c420ec4d67",
   "metadata": {},
   "source": [
    "### Building correspondence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1130354-de46-40b5-ad66-3bd26b00a8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_correspondence_matrix = []\n",
    "features_matched = []\n",
    "\n",
    "for i in range(num_images):\n",
    "    feature = []\n",
    "    matches = []\n",
    "    for j in range(num_features):\n",
    "        feature.append([-1, -1])\n",
    "        matches.append(False)\n",
    "    feature_correspondence_matrix.append(feature)\n",
    "    features_matched.append(matches)\n",
    "\n",
    "for i in range(num_features):\n",
    "    feature_correspondence_matrix[0][i] = feature_data[0][i]\n",
    "    features_matched[0][i] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7c096-9214-497d-8fd6-bd81fd53c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w = img2.shape[:2]\n",
    "ambig_param = 1.1\n",
    "\n",
    "def match_feature(feature, features_to_match, F, distance_threshold=10):\n",
    "    \"\"\"\n",
    "    Matches feature to that of another view, or returns (1, [-1, -1]) if feature was not found, (0, [-1, -1]) and (0, [0, 0]) in case of bad view or ambiguity respectively.\n",
    "    \"\"\"\n",
    "    if feature[0] == -1:\n",
    "        return (1, [-1, -1])\n",
    "\n",
    "    x1 = np.array([feature[1], feature[0], 1])\n",
    "    epipolar_line = F @ x1\n",
    "    epipolar_line = [epipolar_line[0]/epipolar_line[2], epipolar_line[1]/epipolar_line[2], 1.0]\n",
    "\n",
    "    x0, y0 = 0, -epipolar_line[2] / epipolar_line[1]\n",
    "    x1, y1 = h, -(epipolar_line[2] + epipolar_line[0] * h) / epipolar_line[1]\n",
    "    if abs(y0) > (h+w) * 2 or abs(y1) > (h+w) * 2:\n",
    "        return (0, [-1, -1])\n",
    "    \n",
    "    best_match = -1\n",
    "    min_distance = float('inf')\n",
    "    ambiguity = False\n",
    "    \n",
    "    for l in range(len(features_to_match)):\n",
    "        if features_to_match[l, 0] != -1:\n",
    "            x2 = np.array([features_to_match[l, 1], features_to_match[l, 0], 1])\n",
    "            distance = np.abs(epipolar_line[0] * x2[0] + epipolar_line[1] * x2[1] + epipolar_line[2]) / np.sqrt(epipolar_line[0]**2 + epipolar_line[1]**2)\n",
    "            if abs(distance-min_distance) < ambig_param:\n",
    "                ambiguity = True\n",
    "                \n",
    "            if distance < min_distance:\n",
    "                if abs(distance-min_distance) >= ambig_param:\n",
    "                    ambiguity = False\n",
    "                min_distance = distance\n",
    "                best_match = l\n",
    "\n",
    "    if ambiguity:\n",
    "        return (0, [0, 0])\n",
    "    if min_distance < distance_threshold:\n",
    "        return (1, features_to_match[best_match])\n",
    "        \n",
    "    return (1, [-1, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de911902-31d0-4716-a963-1269b780a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_correspondences(view_idx):\n",
    "    transformations = get_transformation_matrices(view_idx)\n",
    "    for i in range(num_images):\n",
    "        if i == view_idx:\n",
    "            continue\n",
    "\n",
    "        images_matched = True\n",
    "        for j in range(num_features):\n",
    "            if features_matched[i][j] == False:\n",
    "                images_matched = False\n",
    "        if images_matched:\n",
    "            continue\n",
    "        \n",
    "        R = transformations[i][:3, :3]\n",
    "        t = transformations[i][:3, 3]\n",
    "        E = get_essential_matrix(R, t)\n",
    "        F = get_fundamental_matrix(E, K)\n",
    "\n",
    "        correspondences = []\n",
    "        matched = []\n",
    "        bad_view = False\n",
    "        \n",
    "        for j in range(num_features):\n",
    "            feature = match_feature(feature_correspondence_matrix[view_idx][j], feature_data[i], F)\n",
    "            if feature[0] == 0 and feature[1][0] == -1:\n",
    "                bad_view = True\n",
    "                break\n",
    "            elif feature[0] == 0:\n",
    "                correspondences.append([-1, -1])\n",
    "                matched.append(False)\n",
    "            else:\n",
    "                correspondences.append(feature[1])\n",
    "                matched.append(True)\n",
    "        \n",
    "        if bad_view:\n",
    "            continue\n",
    "\n",
    "        for j in range(num_features):\n",
    "            if features_matched[i][j] == False:\n",
    "                feature_correspondence_matrix[i][j] = correspondences[j]\n",
    "                features_matched[i][j] = matched[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c836a96-4607-4429-81d2-c0033ffecaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = True\n",
    "for i in range(num_images):\n",
    "    find_missing_correspondences(i)\n",
    "    done = True\n",
    "    for k in range(num_images):\n",
    "        for j in range(num_features):\n",
    "            if features_matched[k][j] == False:\n",
    "                done = False\n",
    "                break\n",
    "    if done:\n",
    "        break    \n",
    "\n",
    "for i in range(len(feature_correspondence_matrix)):\n",
    "    print(feature_correspondence_matrix[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b9b005-5f9a-4143-a497-68e745966c5e",
   "metadata": {},
   "source": [
    "## Finally, see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a70da4-81fb-4a18-ad7b-76d44172bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_images):\n",
    "    img = cv2.imread(undistorted_images[i])\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    \n",
    "    k = 0\n",
    "    for feature in feature_correspondence_matrix[i]:\n",
    "        if feature[0] != -1:\n",
    "            plt.plot(feature[1], feature[0], colors[k] + 'o', markersize=5)\n",
    "        k += 1\n",
    "        \n",
    "    plt.title(f\"Image {i}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "860598ee-d6da-4e6f-8e0e-2b0e59bf9fb7",
   "metadata": {},
   "source": [
    "--- Feature matching prepared by Daiana Tei"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d62c9e6-933e-4ac3-b491-8e8f0ec9aeb5",
   "metadata": {},
   "source": [
    "# please proceed from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945a253-1acd-400f-949d-25bc213e4188",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = [[camera_calibration['focal_length_px'], 0, camera_calibration['principal_point'][1]],\n",
    "    [0, camera_calibration['focal_length_px'], camera_calibration['principal_point'][0]],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "def triangulate(K, view1_index, view2_index, x1, x2):\n",
    "    \"\"\" \n",
    "    Triangulate the 3D point from two camera views.\n",
    "    Args:\n",
    "        K: Camera matrix (3x3)\n",
    "        Transform: Transformation matrix (4x4)\n",
    "        x1: 2D point in view 1 (2,)\n",
    "        x2: 2D point in view 2 (2,)\n",
    "    Returns:\n",
    "        X: 3D point (4,)\n",
    "    \"\"\"\n",
    "    # Get the projection matrices for the two views\n",
    "\n",
    "    P1 = get_projection_matrix(K, view1_index)\n",
    "    P2 = get_projection_matrix(K, view2_index)\n",
    "\n",
    "    u1, v1 = x1\n",
    "    u2, v2 = x2\n",
    "    \n",
    "    A = np.array([\n",
    "        u1 * P1[2] - P1[0],\n",
    "        v1 * P1[2] - P1[1],\n",
    "        u2 * P2[2] - P2[0],\n",
    "        v2 * P2[2] - P2[1]\n",
    "    ])\n",
    "    \n",
    "    _, _, V = np.linalg.svd(A)\n",
    "\n",
    "    X = V[-1]\n",
    "    X = X / X[-1]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986c40cb-6749-4db3-984c-83abf6c39753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection_matrix1(K, view_index):\n",
    "    \"\"\"\n",
    "    Get the camera projection matrices for two views.\n",
    "    Args:\n",
    "        K: Camera intrinsic matrix (3x3)\n",
    "        view1_index: Index of the first view\n",
    "    Returns:\n",
    "        P: Projection matrix of view 1 (3x4)\n",
    "    \"\"\"\n",
    "\n",
    "    # Assume camera_movement is a global variable or passed into the function\n",
    "\n",
    "    global camera_movement\n",
    "    # Identity matrix for the initial view (view 0)\n",
    "    T = np.eye(4)\n",
    "    for i in range(view_index):\n",
    "        T = camera_movement[i] @ T\n",
    "\n",
    "    # Extract R and T for each view\n",
    "    print(T, 'T')\n",
    "    R, T_vec = T[:3, :3], T[:3, 3]\n",
    "\n",
    "    # Projection matrices\n",
    "\n",
    "    P = K @ np.hstack((R, T_vec.reshape(-1, 1)))\n",
    "\n",
    "    return P\n",
    "\n",
    "def get_projection_matrix2(K, view_index):\n",
    "    \"\"\"\n",
    "    Get the camera projection matrices for two views.\n",
    "    Args:\n",
    "        K: Camera intrinsic matrix (3x3)\n",
    "        view1_index: Index of the first view\n",
    "    Returns:\n",
    "        P: Projection matrix of view 1 (3x4)\n",
    "    \"\"\"\n",
    "\n",
    "    T = get_transformation_matrices(view_index)[0]\n",
    "\n",
    "    # Extract R and T for each view\n",
    "    print(get_transformation_matrices(view_index)[0], 'Daiana')\n",
    "    \n",
    "    R, T_vec = T[:3, :3], T[:3, 3]\n",
    "\n",
    "    # Projection matrices\n",
    "\n",
    "    P = K @ np.hstack((R, T_vec.reshape(-1, 1)))\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea60c06e-8ba8-4eba-a287-6f8fb099fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_projection_matrix(K, view_index):\n",
    "    \"\"\"\n",
    "    Get the camera projection matrices for two views.\n",
    "    Args:\n",
    "        K: Camera intrinsic matrix (3x3)\n",
    "        view1_index: Index of the first view\n",
    "    Returns:\n",
    "        P: Projection matrix of view 1 (3x4)\n",
    "    \"\"\"\n",
    "\n",
    "    # Assume camera_movement is a global variable or passed into the function\n",
    "\n",
    "    global camera_movement\n",
    "    # Identity matrix for the initial view (view 0)\n",
    "    T = np.eye(4)\n",
    "    for i in range(view_index):\n",
    "        T = camera_movement[i] @ T\n",
    "\n",
    "    #print(T, 'T')\n",
    "    #print(get_transformation_matrices(view_index)[0], 'Daiana')\n",
    "    # Extract R and T for each view\n",
    "\n",
    "    #T = get_transformation_matrices(view_index)[0]\n",
    "    R, T_vec = T[:3, :3], T[:3, 3]\n",
    "\n",
    "    # Projection matrices\n",
    "\n",
    "    P = K @ np.hstack((R, T_vec.reshape(-1, 1)))\n",
    "\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d983a430-e1c9-4998-afcb-03c7fee11617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2efaf44-be8e-4507-9ffe-1c043f313065",
   "metadata": {},
   "outputs": [],
   "source": [
    "for view1_index in range(0, 9):\n",
    "    for view2_index in range(view1_index+1, 9):\n",
    "        \n",
    "        feature_0_view1 = feature_correspondence_matrix[view1_index][0]\n",
    "\n",
    "        feature_0_view2 = feature_correspondence_matrix[view2_index][0]\n",
    "\n",
    "       # print(view1_index, view2_index, feature_0_view1, feature_0_view2)\n",
    "        \n",
    "        # Triangulate the 3D point\n",
    "\n",
    "        X = triangulate(K, view1_index, view2_index, feature_0_view1, feature_0_view2)\n",
    "\n",
    "        print(view1_index, view2_index, feature_0_view1, feature_0_view2, X)\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a87f0-453f-4d61-a77a-d39d5f70ea49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_reprojection_error(points_3d, points_2d, projection_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the reprojection error for a set of 3D points and their corresponding 2D points.\n",
    "\n",
    "    Parameters:\n",
    "        points_3d (numpy.ndarray): N x 4 array of 3D points in homogeneous coordinates.\n",
    "        points_2d (numpy.ndarray): N x 2 array of observed 2D points in the image.\n",
    "        projection_matrix (numpy.ndarray): 3x4 camera projection matrix.\n",
    "\n",
    "    Returns:\n",
    "        float: The average reprojection error across all points.\n",
    "    \"\"\"\n",
    "    num_points = points_3d.shape[0]\n",
    "    total_error = 0\n",
    "\n",
    "    for i in range(num_points):\n",
    "        # Project the 3D point to the image plane\n",
    "        projected = projection_matrix @ points_3d[i]  # Matrix multiplication\n",
    "        u_proj = projected[0] / projected[2]  # Normalize x-coordinate\n",
    "        v_proj = projected[1] / projected[2]  # Normalize y-coordinate\n",
    "\n",
    "        # Observed 2D point\n",
    "        u_obs, v_obs = points_2d[i]\n",
    "\n",
    "        # Compute Euclidean distance (reprojection error)\n",
    "        error = np.sqrt((u_obs - u_proj)**2 + (v_obs - v_proj)**2)\n",
    "        total_error += error\n",
    "\n",
    "    # Return average reprojection error\n",
    "    return total_error / num_points\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 3D points (homogeneous coordinates)\n",
    "    points_3d = np.array([\n",
    "        [-1.48005423,  2.99432544,  4.00321339,  1.]\n",
    "    ])\n",
    "\n",
    "    # Corresponding 2D observed points\n",
    "    points_2d = np.array([\n",
    "        [163, 616],\n",
    "        [183, 694],\n",
    "        [157, 501]\n",
    "    ])\n",
    "\n",
    "    # Example camera projection matrix\n",
    "    projection_matrix = get_projection_matrix(K, 0)\n",
    "    # Calculate reprojection error\n",
    "    error = calculate_reprojection_error(points_3d, points_2d, projection_matrix)\n",
    "    print(f\"Average Reprojection Error: {error:.4f} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac93c7-5f89-4b8d-a3bc-2c514ad57a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "Multiview Triangulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d250a2e6-7a4b-40a1-bebe-747c8b8e2704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def multiview_triangulation(points_2d, projection_matrices):\n",
    "    \"\"\"\n",
    "    Perform multiview triangulation using all views.\n",
    "    \n",
    "    Parameters:\n",
    "        points_2d (list of numpy.ndarray): List of 2D points (one per view) in homogeneous coordinates.\n",
    "        projection_matrices (list of numpy.ndarray): List of 3x4 projection matrices (one per view).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The triangulated 3D point in Euclidean coordinates.\n",
    "    \"\"\"\n",
    "    # Number of views\n",
    "    num_views = len(points_2d)\n",
    "    \n",
    "    # Construct matrix A\n",
    "    A = []\n",
    "    for i in range(num_views):\n",
    "        P = projection_matrices[i]\n",
    "        x, y, w = points_2d[i]\n",
    "        \n",
    "        # Add two rows for each view\n",
    "        A.append(x * P[2, :] - P[0, :])  # First row\n",
    "        A.append(y * P[2, :] - P[1, :])  # Second row\n",
    "    \n",
    "    A = np.array(A)  # Convert to NumPy array\n",
    "    \n",
    "    # Solve A * X = 0 using SVD\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    X_homogeneous = Vt[-1]  # Last row of Vt corresponds to smallest singular value\n",
    "    \n",
    "    # Convert to Euclidean coordinates\n",
    "    X_euclidean = X_homogeneous[:-1] / X_homogeneous[-1]\n",
    "    return X_euclidean\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example 2D points in homogeneous coordinates (one per view)\n",
    "    points_2d = [\n",
    "        np.array([163, 616, 1]),\n",
    "        np.array([183, 694, 1]),\n",
    "        #np.array([157, 501, 1]),\n",
    "        #np.array([199, 614, 1]),\n",
    "       # np.array([168, 693, 1]),\n",
    "        #np.array([207, 500, 1]),\n",
    "        #np.array([177, 610, 1]),\n",
    "        #np.array([199, 614, 1]),\n",
    "        #np.array([199, 614, 1])\n",
    "    ]\n",
    "    \n",
    "    # Example projection matrices (one per view)\n",
    "    projection_matrices = [\n",
    "        get_projection_matrix(K, 0),\n",
    "        get_projection_matrix(K, 1),\n",
    "        #get_projection_matrix(K, 2),\n",
    "       # get_projection_matrix(K, 3),\n",
    "        #get_projection_matrix(K, 4),\n",
    "        #get_projection_matrix(K, 5),\n",
    "        #get_projection_matrix(K, 6),\n",
    "        #get_projection_matrix(K, 7),\n",
    "        #get_projection_matrix(K, 8)\n",
    "    ]\n",
    "    \n",
    "    # Perform triangulation\n",
    "    point_3d = multiview_triangulation(points_2d, projection_matrices)\n",
    "    print(f\"Triangulated 3D Point: {point_3d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986a7e5-b1bc-4709-9901-213ca70cf2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad0027c-c3f2-42fe-ac79-227f1a0af768",
   "metadata": {},
   "outputs": [],
   "source": [
    "Triangulated 3D Point: [-1.62395939  2.27025613  4.28017569]\n",
    "Triangulated 3D Point: [-1.52852253  2.25124501  4.36313952]\n",
    "Triangulated 3D Point: [-1.582235    2.53947771  4.43063668]\n",
    "Triangulated 3D Point: [-1.60952381  2.4201067   4.58861689]\n",
    "Triangulated 3D Point: [-1.70029089  2.6601052   4.65973963]\n",
    "Triangulated 3D Point: [-1.48005423  2.99432544  4.00321339]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01781f25-ffc3-40e7-adc4-eb3be1b1aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate all 3d points with multiview triangulation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def multiview_triangulation(points_2d, projection_matrices):\n",
    "    \"\"\"\n",
    "    Perform multiview triangulation using all views.\n",
    "    \n",
    "    Parameters:\n",
    "        points_2d (list of numpy.ndarray): List of 2D points (one per view) in homogeneous coordinates.\n",
    "        projection_matrices (list of numpy.ndarray): List of 3x4 projection matrices (one per view).\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: The triangulated 3D point in Euclidean coordinates.\n",
    "    \"\"\"\n",
    "    num_views = len(points_2d)\n",
    "    A = []\n",
    "    \n",
    "    for i in range(num_views):\n",
    "        P = projection_matrices[i]\n",
    "        x, y, w = points_2d[i]\n",
    "        \n",
    "        # Add two rows for each view\n",
    "        A.append(x * P[2, :] - P[0, :])  # Row from x-coordinate\n",
    "        A.append(y * P[2, :] - P[1, :])  # Row from y-coordinate\n",
    "    \n",
    "    A = np.array(A)  # Convert to NumPy array\n",
    "    \n",
    "    # Solve A * X = 0 using SVD\n",
    "    _, _, Vt = np.linalg.svd(A)\n",
    "    X_homogeneous = Vt[-1]  # Last row of Vt corresponds to smallest singular value\n",
    "    \n",
    "    # Convert to Euclidean coordinates\n",
    "    X_euclidean = X_homogeneous[:-1] / X_homogeneous[-1]\n",
    "    return X_euclidean\n",
    "\n",
    "# Example loop for 7 features from 9 views\n",
    "if __name__ == \"__main__\":\n",
    "    # Example data: Replace with your actual 2D points and projection matrices\n",
    "    # Simulated 2D points for 7 features across 9 views\n",
    "    points_2d_all_features = [\n",
    "        [  # Feature 1 points in 9 views\n",
    "            np.array([320, 240, 1]),\n",
    "            np.array([315, 235, 1]),\n",
    "            np.array([310, 230, 1]),\n",
    "            np.array([305, 225, 1]),\n",
    "            np.array([300, 220, 1]),\n",
    "            np.array([295, 215, 1]),\n",
    "            np.array([290, 210, 1]),\n",
    "            np.array([285, 205, 1]),\n",
    "            np.array([280, 200, 1])\n",
    "        ],\n",
    "        # Add similar blocks for Feature 2 through Feature 7\n",
    "        # Each feature should have 9 points in homogeneous coordinates\n",
    "    ]\n",
    "    \n",
    "    # Example projection matrices for 9 views\n",
    "    projection_matrices = [get_projection_matrix(x) for x in range(0,9)]\n",
    "    \n",
    "    # Triangulate all 7 features\n",
    "    points_3d_all_features = []\n",
    "    \n",
    "    for feature_points in points_2d_all_features:\n",
    "        # Triangulate this feature using multiview triangulation\n",
    "        point_3d = multiview_triangulation(feature_points, projection_matrices)\n",
    "        points_3d_all_features.append(point_3d)\n",
    "    \n",
    "    # Print the 3D coordinates of all features\n",
    "    for i, point_3d in enumerate(points_3d_all_features):\n",
    "        print(f\"Feature {i + 1}: 3D Coordinate = {point_3d}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fda35-643e-4a71-af00-fd6f3b55ff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_matrices = [get_projection_matrix(K, x) for x in range(0,9)]\n",
    "projection_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d527e1f4-a70b-4f38-9afd-8e409cd60573",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_matrices = [\n",
    "        np.array([[1000, 0, 320, 0],\n",
    "                  [0, 1000, 240, 0],\n",
    "                  [0, 0, 1, 0]]),\n",
    "        \n",
    "        np.array([[950, 0, 300, -50],\n",
    "                  [0, 950, 230, -20],\n",
    "                  [0, 0, 1, 0]]),\n",
    "        \n",
    "        np.array([[900, 0, 280, -100],\n",
    "                  [0, 900, 220, -30],\n",
    "                  [0, 0, 1, 0]]),\n",
    "        \n",
    "        # Add similar projection matrices for the remaining views (up to 9 views)\n",
    "    ]\n",
    "projection_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852b2ca9-8c03-4088-9832-62708f1fc802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
