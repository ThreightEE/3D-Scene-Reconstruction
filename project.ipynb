{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7dc1b1",
   "metadata": {},
   "source": [
    "# 3D Computer Vision (2024/25)\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Upload: 30.10.2024 (11:30)\n",
    "\n",
    "**Deadline**: 16.01.2025 (23:59)\n",
    "\n",
    "### Your Group\n",
    "Submitted by Group 07: \n",
    "- Daiana Tei\n",
    "- Name2\n",
    "- Name3\n",
    "- Name4\n",
    "\n",
    "By submitting this exercise, you confirm the following:\n",
    "- **All people** listed above **contributed** to this solution\n",
    "- **No other people** were **involved** in this solution\n",
    "- **No contents** of this solution were **copied from others** (this includes people, large language models, websites, etc.)\n",
    "\n",
    "### Submission\n",
    "Please hand in a single **.zip** file named according to the pattern \"**groupXX**\" (e.g. group00). The contents of the .zip should be as follows:\n",
    "- folder with the same name as the **.zip** file\n",
    "    - **.ipynb** file\n",
    "    - **.html** export of .ipynb with all the outputs you got\n",
    "    - **data** folder containing necessary files to run the code\n",
    "\n",
    "I.e.\n",
    "1. **unzip** the provided project.zip file\n",
    "2. **rename** folder \"project\" according to the pattern \"groupXX\"\n",
    "3. **solve** task inside .ipynb file\n",
    "4. **export** notebook as .html (File > Download as > HTML)\n",
    "5. **zip** folder groupXX\n",
    "6. **submit** groupXX.zip\n",
    "\n",
    "### Final Presentation\n",
    "You will be required to present your solution in a 20 minute presentation, which includes:\n",
    "- Problem Overview\n",
    "- Solution Overview (e.g. pseudo code, mathematical formulas, visualizations)\n",
    "- Describe challenges & optimizations\n",
    "\n",
    "After the presentation, there will be 10 minutes of questions and answers about your work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f337c5fd",
   "metadata": {},
   "source": [
    "# 3D Scene Reconstruction\n",
    "### Task Overview\n",
    "Your task in this exercise is to do a dense reconstruction of a scene. This will involve multiple steps that you will encounter and learn about as the semester progresses. You can start implementing individual steps as soon as you learn about them or wait until you have learned more to implement everything together. In the latter case, be mindful that this exercise is designed for an entire semester and the workload is accordingly large.\n",
    "\n",
    "You will be given the following data:\n",
    "- **9 color images** of the scene.\n",
    "    - 8 Bit RGB per pixel.\n",
    "    - Each image rendered from a different position.\n",
    "    - The camera used had **lens distortion**.\n",
    "- **9 Depth images** of the scene.\n",
    "    - 8 Bit Grayscale per pixel. The result of dividing the Z-depth by **each image's maximum** and then multiplying by 255.\n",
    "    - Each image has the **same pose** as the corresponding RGB image.\n",
    "    - The camera used was **free of any distortions**.\n",
    "- 1 Dictionary containing **camera calibration parameters**.\n",
    "    - They belong to the camera that was used to render the RGB images.\n",
    "    - Distortion coefficients are given in the standard [k<sub>1</sub>, k<sub>2</sub>, p<sub>1</sub>, p<sub>2</sub>, k<sub>3</sub>] order.\n",
    "- 1 Numpy array containing **8 camera transformations**.\n",
    "    - They specify the **movements** that the **camera went through** to render all images.\n",
    "    - I.e. idx **0** specifies the transformation from **00.png to 01.png**, idx **1** specifies the transformation from **01.png to 02.png**, ...\n",
    "    - This applies to both RGB and Depth images, as they have the same poses.\n",
    "- 1 Numpy array containing **7 features**.\n",
    "    - The features are specified for each of the 9 images.\n",
    "    - Each feature is a **2D pixel location in \"H, W\" order**, meaning the first value is the height/row in the image and the second width/column.\n",
    "    - If a feature was not visible, it was entered as [-1, -1].\n",
    "    - The features are **unsorted**, meaning that feature idx 0 for 00.png could be corresponding to e.g. feature idx 4 for 01.png.\n",
    "\n",
    "### Solution requirements\n",
    "- Your code needs to **compile**, **run**, and produce an **output**.\n",
    "- Your target output should be a **dense point cloud** reconstruction (without holes) of the scene.\n",
    "    - The output should be in the **.ply format**. We provide a function that can exports a .ply file.\n",
    "    - You may inspect your .ply outputs in e.g. **Meshlab**.\n",
    "    - See the 'Dense Point Cloud' sample image to get an idea of what is possible. (Meshlab screenshot with point shading set to None)\n",
    "- Your code should be a **general solution**.\n",
    "    - This means that it could run correctly for a different dataset (with same input structure).\n",
    "    - It should **NOT** include anything **hardcoded** specific to this dataset.\n",
    "- Your code should not be unnecessarily inefficient.\n",
    "    - Our sample solution runs in less than 2 minutes total (including point cloud export).\n",
    "    - If your solution runs for more than 10 minutes, you are being wasteful in some part of your program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b718a",
   "metadata": {},
   "source": [
    "# Samples\n",
    "### Image Distortion\n",
    "![title](data/samples/image_distortion.png)\n",
    "### Cameras\n",
    "![title](data/samples/cameras.png)\n",
    "### Cameras & Features\n",
    "![title](data/samples/features.png)\n",
    "### Dense Point Cloud\n",
    "![title](data/samples/dense_point_cloud.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bde4af",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Please note the following:\n",
    "- These are all imports necessary to achieve the sample results.\n",
    "- You may remove and/or add other libraries at your own convinience.\n",
    "- Using library functions (from the given or other libraries) that bypass **necessary computer vision tasks** will not be recognized as 'solved'.\n",
    "    - E.g.: If you **need** to undistort an image to **get to the next step** of the solution and use the library function cv2.undistort(), then we will evaluate the **undistortion step** as '**failed**'.\n",
    "    - E.g.: If you **want** to draw points in an image (to **check your method** or **visualize in-between steps**) and use the library function cv2.circle(), then there is **no problem**.\n",
    "    - E.g.: If you **need** to perform complex **mathematical** operations and use some numpy function, then there is **no problem**.\n",
    "    - E.g.: You do not like a **provided utility function** and find/know a library function that gives the **same outputs** from the **same inputs**, then there is **no problem**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b8dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#make plots interactive:\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68e1e4",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "This should load all available data and also create some output directories. Feel free to rename variables or add additional directories as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85509287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs\n",
    "base_path = os.getcwd()\n",
    "data_path = os.path.join(base_path, f\"data\")\n",
    "img_path = os.path.join(data_path, 'images')\n",
    "depth_path = os.path.join(data_path, 'depths')\n",
    "print(f\"The project's root path is '{base_path}'.\")\n",
    "print(f\"Reading data from '{data_path}'.\")\n",
    "print(f\"Image folder: '{img_path}'.\")\n",
    "print(f\"Depth folder: '{depth_path}'.\")\n",
    "\n",
    "#Outputs\n",
    "out_path = os.path.join(base_path, 'output')\n",
    "ply_path = os.path.join(out_path, 'point_cloud')\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "os.makedirs(ply_path, exist_ok=True)\n",
    "print(f\"\\nCreating directory '{out_path}'.\")\n",
    "print(f\"Creating directory '{ply_path}'.\")\n",
    "\n",
    "#Load Data\n",
    "camera_calibration = np.load(os.path.join(data_path, 'camera_calibration.npy'), allow_pickle=True)\n",
    "camera_calibration = camera_calibration.item()#get dictionary from numpy array struct\n",
    "given_features = np.load(os.path.join(data_path, 'given_features.npy'), allow_pickle=True)\n",
    "camera_movement = np.load(os.path.join(data_path, 'camera_movement.npy'), allow_pickle=True)\n",
    "\n",
    "print(f\"\\nCamera Calibration:\")\n",
    "for entry in camera_calibration.items():\n",
    "    print(f\"  {entry[0]}: {entry[1]}\")\n",
    "print(f\"Camera Movement: {camera_movement.shape}\")#[Cameras-1, 4, 4]\n",
    "print(f\"2D Features (Unsorted): {given_features.shape}\")#[Camera_idx, Feature_idx, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b491e536",
   "metadata": {},
   "source": [
    "# Provided Utility Functions\n",
    "These functions are provided to reduce the complexity of some steps you might encounter. They were involved in the creation of the given samples. However, you do not have to use them and can use other means of achieving the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e69911b-e5df-49dc-8dd0-37a8af1c1539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(numpy_image, numpy_sample_grid):\n",
    "    '''\n",
    "    This function samples a target image from a source image (numpy_image) based on specified pixel coordinates (numpy_sample_grid).\n",
    "    Inputs:\n",
    "        numpy_image: of shape=[H, W, C]. H is the height, W is the width, and C is the color channel of the source image from which color values will be sampled.\n",
    "        numpy_sample_grid: of shape=[H, W, UV]. H is the height and W is the width of the target image that will be sampled. UV are the pixel locations in the source image from which to sample color values.\n",
    "    Outputs:\n",
    "        sampled_image: of shape=[H, W, C]. H is the height, W is the width, and C is the color channel of the target image that was sampled.\n",
    "    '''\n",
    "    height, width, _ = numpy_image.shape#[H, W, 3]\n",
    "\n",
    "    #turn numpy array to torch tensor\n",
    "    torch_sample_grid = torch.from_numpy(numpy_sample_grid)#[H, W, 2]\n",
    "    #normalize from range (0, width-1) to (0, 1)\n",
    "    torch_sample_grid[:, :, 0] = torch_sample_grid[:, :, 0] / (width-1)\n",
    "    #normalize from range (0, height-1) to (0, 1)\n",
    "    torch_sample_grid[:, :, 1] = torch_sample_grid[:, :, 1] / (height-1)\n",
    "    #normalize from range (0, 1) to (-1, 1)\n",
    "    torch_sample_grid = torch_sample_grid*2 -1\n",
    "\n",
    "    #transform to necessary shapes\n",
    "    torch_sample_grid = torch_sample_grid.unsqueeze(0)#[1, H, W, 2]\n",
    "    torch_image = torch.from_numpy(numpy_image).double().permute(2, 0, 1).unsqueeze(0)#[1, 3, H, W]\n",
    "    #sample image according to sample grid locations from source image\n",
    "    sampled_image = torch.nn.functional.grid_sample(torch_image, torch_sample_grid, mode='bilinear', padding_mode='zeros', align_corners=True)\n",
    "    #transform back to numpy image\n",
    "    sampled_image = sampled_image.squeeze().permute(1, 2, 0).numpy().astype(np.uint8)#[H, W, 3]\n",
    "    return sampled_image\n",
    "\n",
    "def ply_creator(input_3d, rgb_data=None, filename='dummy'):\n",
    "    ''' Creates a colored point cloud that you can visualise using e.g. Meshlab.\n",
    "    Inputs:\n",
    "        input_3d: of shape=[N, 3], each row is the 3D coordinate of each point\n",
    "        rgb_data(optional): of shape=[N, 3], each row is the rgb color value of each point\n",
    "        filename: file name for the .ply file to be created \n",
    "    '''\n",
    "    assert (input_3d.ndim==2),\"Pass 3d points as NumPointsX3 array \"\n",
    "    pre_text1 = \"\"\"ply\\nformat ascii 1.0\"\"\"\n",
    "    pre_text2 = \"element vertex \"\n",
    "    pre_text3 = \"\"\"property float x\\nproperty float y\\nproperty float z\\nproperty uchar red\\nproperty uchar green\\nproperty uchar blue\\nend_header\"\"\"\n",
    "    pre_text22 = pre_text2 + str(input_3d.shape[0])\n",
    "    pre_text11 = pre_text1\n",
    "    pre_text33 = pre_text3\n",
    "    filename = filename + '.ply'\n",
    "    fid = open(filename, 'w')\n",
    "    fid.write(pre_text11)\n",
    "    fid.write('\\n')\n",
    "    fid.write(pre_text22)\n",
    "    fid.write('\\n')\n",
    "    fid.write(pre_text33)\n",
    "    fid.write('\\n')\n",
    "    for i in range(input_3d.shape[0]):\n",
    "        for c in range(3):\n",
    "            fid.write(str(input_3d[i,c]) + ' ')\n",
    "        if not rgb_data is None:\n",
    "            for c in range(3):\n",
    "                fid.write(str(rgb_data[i,c]) + ' ')\n",
    "        if i!=input_3d.shape[0]:\n",
    "            fid.write('\\n')\n",
    "    fid.close()\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following is a simple toy example to see the behavior of the sample_image function\n",
    "example_source = (np.random.rand(110, 220, 3)*255).astype(int)#[110, 220, 3]\n",
    "example_grid = np.ones([100, 200, 2])#[100, 200, 2]\n",
    "example_grid[:, :, 1] = 2\n",
    "example_target = sample_image(example_source, example_grid)#[100, 200, 3]\n",
    "\n",
    "#example_target will be of shape [100, 200, 3]\n",
    "#100, because example_grid has a height of 100\n",
    "#200, because example_grid has a width of 200\n",
    "#3, because example_source has a color channel of 3\n",
    "print(example_source.shape)\n",
    "\n",
    "#example_target will contain the value of example_source[2, 1] at every pixel\n",
    "#2, because example_grid[:, :, 1] has a value of 2 for every pixel\n",
    "#1, because example_grid[:, :, 0] has a value of 1 for every pixel\n",
    "print(example_source[2, 1, 0], \"->\", np.unique(example_target[:, :, 0]))\n",
    "print(example_source[2, 1, 1], \"->\", np.unique(example_target[:, :, 1]))\n",
    "print(example_source[2, 1, 2], \"->\", np.unique(example_target[:, :, 2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa9914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
